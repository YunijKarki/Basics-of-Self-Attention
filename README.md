
# Understanding The Basics of Self Attention

Self Attention is basically the driving wheel of Transformers which is considered state-of-the-art in various natural language processing (NLP) and machine learning tasks. So, here's my simple explaination of what's going under the hood in transformers. Self Attention is quite complex topic on its own. But i have made sure that it is understandable and once you get through the notebook, you'll get a clear idea of what's happening underneath such powerful models.


